---
layout:     post
title:      "Big Data Review"
subtitle:   " \"CMSC5741 Review Notes\""
date:       2020.11.09 23:29:00
author:     "Wuy"
header-img: "img/bigdata.jpg"
catalog: true
tags:
    - Big Data

---

> *"Keep Learning Big Data"*

# 01. Introduction

1. Characteristics of Big Data: Volume, Velocity, Variety, Veracity.
2. Definition of Big Data Analytics: A process of inspecting, cleaning, transforming, and modeling big data with the goal of discovering useful information, suggesting conclusions, and supporting decision making.
3. Pitfall in big data analytics: 邦弗朗尼原理：假定人们有一定量的数据并期望从该数据中找到某个特定类型的事件，即使数据完全随机，也可以期望该类型事件会发生。 邦弗朗尼校正定理给出一个统计学上可行的方法来避免在搜索数据时出现的大部分“臆造”的正响应。 例如：如果考察的时间和范围过广，会很容易发现一些人同住一家酒店，而两者没有什么关系。

# 02. MapReduce

1. MapReduce 存储架构：HDFS、GFS，下图为GFS架构示例：

   ![image-20201108000330645](https://i.loli.net/2020/11/08/rH1j6GMTCO5AUmw.png)

2. The Hadoop Distributed File System (HDFS) is a natural solution to cope with the scale and failure faced by large clusters：

   - Distributed File System（A HDFS instance may consist of thousands of server machines, each storing **part** of the file system’s data.）

   - Provides global file namespace

   - Replica to ensure data recovery

   - Detection of faults in thousands of components and quick, automatic recovery from them are a core architectural goal of HDFS 

3. Data Characteristics:

   - Streaming data access
   - Batch processing rather than interactive user access
   - Write-once-read-many: a file, once created, written and closed, need not be changed

4. HDFS Architecture: NameNode is a file system namespace allow user to store files in it. DataNodes store splited blocks of files, and serves read, write requests, performs block creation, deletion, and replication upon instruction from NameNode：

   ![image-20201108113931742](https://i.loli.net/2020/11/08/E6jQslCa53ymci4.png)

5. Data Replication：

   - Each file is a sequence of blocks.
   - All blocks in the file, except the last, are of the same size.
   - Blocks are replicated for fault tolerance.
   - Block size and replicas are configurable per file.

6. Principles of replica selection: 尽量减少带宽消耗和延迟、优先选择可读节点、尽量选择本地数据中心（如果有远程数据中心）.

7. Safemode of NameNode: 安全模式是HDFS所处的一种特殊状态，在这种状态下，文件系统只接受读数据请求，而不接受删除、修改等变更请求。在NameNode主节点启动时，HDFS首先进入安全模式，DataNode在启动的时候会向namenode汇报可用的block等状态，当整个系统达到安全标准时，HDFS自动离开安全模式。如果HDFS出于安全模式下，则文件block不能进行任何的副本复制操作，因此达到最小的副本数量要求是基于datanode启动时的状态来判定的，启动时不会再做任何复制（从而达到最小副本数量要求）.

8. NameNode keeps image of entire file system namespace(FsImage) and Editlog, and stores the copy of FsImage as a checkpoint in case of crash. 4GB of local RAM is sufficient to store the above data structures.

9. DataNode has no knowledge about HDFS filesystem and DataNode does not create all files in the same directory. It only generates reports when the file system starts up and sends its report to the NameNode.

10. MapReduce steps overview: 

    - Sequentially read a lot of data(a set of key-value pairs)
    - Map: Extract something you care about
    - Group by key: shuffle and sort
    - Reduce: Aggregate, summarize, filter or transform
    - Write the result

11. MapReduce workflow(DataNodes also serve as compute servers):

    ![image-20201108150341704](https://i.loli.net/2020/11/08/ye7EZcrJoisRCL8.png)

12. Dataflow:

    - Schedulers tend to schedule tasks "close" to  physical storage location of input data
    - Intermedia results are stored on local file system of Map and Reduce workers
    - Output is often the input of another MapReduce job

13. Mater node:

    - Task status: (idle(闲置), in-progress, completed)

    - Idle tasks get scheduled as workers become available

    - When a map task completes, it sends the master the location and sizes of its $R$ intermediate files, one for each reducer 

    - Master pushes this info to reducers
    - Master ping workers periodly to detect failures

14. Failures: 

    - All completed and in-progress tasks are reset to idle once Map workers fail and Reduce workers will be notified
    - Only in-progress tasks are reset to idle and restarted

15. Rule of mappers/reducers:

    - Make mappers much larger than nodes' amount
    - One chunk per mapper is common
    - Ususally mappers are larger than reducers

16. Combiners:

    - Only if reducers are commutative and associative, we can use combiners to pre-aggregate values in the mapper to save network times

17. Partitioners:

    - Inputs to map tasks are created by contiguous splits of input file
    - Reducer needs to ensure that records with the same intermediate key end up at the same worker
    - System has a default partition function: $Hash(key)\,mod\,R$
    - Sometimes useful to override the hash function: E.g., $Hash(hostname(URL))\;mod\;R$ ensures `URLs` from a host end up in the same output file

18. Hadoop Streaming command distribute the `mapper.py` and `reducer.py` to every task tracker

19. Costmeasure of MapReduce: 

    - Communication cost: input file size + 2 × (sum of the sizes of all files passed from Map processes to Reduce processes) + the sum of the output sizes of the Reduce processes. 
    - Elapsed communication cost: the sum of the largest input + output for any map process, plus the same for any reduce process 
    - (Elapsed) computation cost, which is linear in the input + output size, so it is like communication cost
    - The big-O notation is not always useful(adding more machines is always an option)
    - Elapsed cost is wall-clock time using parallelism

# 03. Frequent Itemsets

1. Computation Model: the true cost is the number of disk $I/O$  and we measure it by the number of passes an algorithm makes over the data
2. A-Priori algorithm:
   - Key idea: monotonicity: If a set of items $I$ appears at least $s$ times, so does every subset $J$ of $I$
   - Contrapositive for pairs: If item $i$ does not appear in $s$ baskets, then no pair including $i$ can appear in $s$ baskets

# 04. Locality Sensitive Hashing

1. **Jaccard相似系数**（*Jaccard similarity coefficient*）又称**Jaccard系数**(*Jaccar Index*): 两个集合A和B的交集元素在A，B的并集中所占的比例，称为两个集合的Jaccard相似系数，用符号$J(A, B)$表示, **Jaccard相似度=Jaccard相似系数**, 与杰卡德相似系数相反的概念是**Jaccard距离(***Jaccard Distance*)。Jaccard相似系数与Jaccard距离之间的关系：Jaccard距离是Jaccard相似系数的补集，被定义为**Jaccard距离=1-Jaccard相似系数**

2. 局部敏感哈希的处理流程示例：

   ![image-20201108202454716](https://i.loli.net/2020/11/08/WXsUB6LmJovpjZi.png)

3. Shingles: A *k-shingle* (or *k-gram*) for a document is a sequence of k tokens that appears in the doc, and shingle can be represented as a set or a bag, normally , we choose a set. k should be relatively large so that they could appear in both two documents rarely

4. Equivalently, each document is a 0/1 vector in the space of *k-shingles*: 

   - Each unique shingle is a dimension
   - Vectors are very sparse

5. Working assumption: Documents that have lots of shingles in common have similar text, even if the text appears in different order

6. Min-Hashing:

   - The goal is to find a hash function that If sim(C_1,C_2) is high, then with high prob. h(C_1 )=h(C_2) and If sim(C_1,C_2) is low, then with high prob. h(C_1 )≠h(C_2)

   - Not all similarity metrics have a suitable hash function

   - Define a “hash” function h_π (C) = the number of the first (in the permuted order π) row in which column *C* has a value 1 and use sevral independent hash functions to create a signature of a column

   - How to prove that minhash function h fulfill that: $Pr[h(C1) = h(C2)] = sim(C1, C2)$ :

     ![image-20201108222510794](https://i.loli.net/2020/11/08/sQ6FLMzWXHPpVBI.png)

7. Tricks in Min-Hashing:

   - Permuting rows even once is prohibitive
   - Universal hashing: h_(a,b) (x)=((a∙x+b)mod p)  mod N, where *a, b* are random integers, *p* is a prime number (*p*>N), and *a* and *p* must be relative prime for a true permutation

8. Locality Sensitive Hashing: 

   - Goal is to find documents with Jaccard similarity at least *s* (for some similarity threshold)
   - General idea: Use a (hash) function *f(**x,y**)* that tells whether x and y is a candidate pair: a pair of elements whose similarity must be evaluated
   - For minhash matrices: Hash columns of signature matrix to many buckets and then each pair of documents that hashes into the same bucket is a candidate pair (for further examination)
   - We divide matrics into b bands with r rows. And for each band, hash its portion of each column to a hash table with k buckets and make k as large as possible so that two vectors hashing to the same bucket only if they are identical
   - Candidate column pairs are those that hashing to the same bucket with at least one band
   - It is only for simplifying analysis, not for the correctness of algorithm

9. Analysis of the LSH:

   Suppose we use b bands of r rows each, and suppose that a particular pair of documents have Jaccard similarity s. Recall from Section 3.3.3 that the probability the minhash signatures for these documents agree in any one particular row of the signature matrix is s. We can calculate the probability that these documents (or rather their signatures) become a candidate pair as follows:

   - The probability that the signatures agree in all rows of one particular band is $s^r$ .
   - The probability that the signatures disagree in at least one row of a particular band is $1 - s^r$.
   - The probability that the signatures disagree in at least one row of each of the bands is $(1 − s^r )^b$ .
   - The probability that the signatures agree in all the rows of at least one band, and therefore become a candidate pair, is $1 - (1 − s^r )^b$
   - If the numer of band and row is low, the number of false positives would go down and number of false negatives would go up
   - We need to check in main memory that **candidate pairs** really do have **similar signatures**

10. LSH函数族：
    - Theory leaves unknown what happens to pairs that are at distance between d_1 and d_2
    - OK for Minhash, others, but must be part of LSH-family definition

# 05. Mining Data Streams

1. Problems on Data Streams: 
   - Sampling data from a stream
   - Queries over sliding windows
   - Filtering a data stream

2. Naive approach: Suppose each user issues s queries once and d queries twice (total of *s*+2d queries), sample rate is p. And the sample-based answer is: $dp^2/(sp + dp^2 + 2p(1-p)d)$

3. Generalized Solution:

   - Pre-Claim: This algorithm maintains a sample *S* with the desired property, i.e., each item is in the sample *S* with equal prob.

   - The probability that tuple is still in S is $s/(n + 1)$, the proof is: 

     ![image-20201109150139363](https://i.loli.net/2020/11/09/m3TDbsqgSOXRfPY.png)

   - 

